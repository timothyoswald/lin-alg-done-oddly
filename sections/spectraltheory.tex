\chapter{Elementary Spectral Theory}

\oldsection{Eigenvalues and Eigenvectors}
Here we will only consider linear transformations $T_A:V\to V$ via $T_A(\vec{x})=A\vec{x}$ where $V$ is an arbitrary vector space over some field $\mathbb{F}$. In general, it is most common for this vector space to be $\mathbb{C}^n$ and we'll discuss the algebraic motivation behind this shortly. The motivation behind this chapter is the fact that often times we need to repeatedly apply linear transformations. This means that we need to compute $A^n\vec{x}$, but this is an expensive computation since matrix multiplication is not trivial to compute. In this chapter, we will analyze when we have "nicer" ways to compute $A^n\vec{x}$ and to what extent we can determine when we are in these scenarios.
\begin{definition}
    Let $A$ be a $n\times n$ matrix. Assume that we have $A\vec{x}=\lambda\vec{x}$ for $\vec{x}\in V$ and $\lambda\in\mathbb{F}$. We call $\lambda$ an eigenvalue of $A$ and $\vec{x}$ a corresponding eigenvector. $\vec{0}$ is not considered an eigenvector.
\end{definition}
Let's analyze when we have a solution for $A\vec{x}=\lambda\vec{x}$.
\begin{align*}
    A\vec{x}&=\lambda\vec{x}\\
    A\vec{x}-\lambda\vec{x}&=\vec{0}\\
    A\vec{x}-\lambda I\vec{x}&=\vec{0}\\
    (A-\lambda I)\vec{x}&=\vec{0}
\end{align*}
If $A-\lambda I$ is invertible, then this equation has exactly one solution: the zero vector. We require that eigenvectors are non-zero so we care about the solutions when $A-\lambda I$ is not invertible. As we know, this is equation has nontrivial solutions exactly when $\det(A-\lambda I)=0$.
\begin{example}
    Let $A=\begin{bmatrix}
        2 & 1 \\
        1 & 2
    \end{bmatrix}$. We have that $A-\lambda I=\begin{bmatrix}
        2 - \lambda & 1\\
        1 & 2 - \lambda
    \end{bmatrix}$ and $\det(A-\lambda I)=(2-\lambda)^2-1$.
    \begin{align*}
        (2-\lambda)^2-1&=0\\
        4-4\lambda+\lambda^2-1&=0\\
        \lambda^2-4\lambda+3&=0\\
        (\lambda -3)(\lambda-1)&=0
    \end{align*}
    This tells us that $1$ and $3$ are eigenvalues for $A$. Now, let's find their corresponding eigenvectors. First, let $\lambda=1$.
    \begin{align*}
        A\vec{x}&=\lambda\vec{x}\\
        A\vec{x}&=\vec{x}\\
        \begin{bmatrix}
            2 & 1\\
            1 & 2
        \end{bmatrix}\begin{bmatrix}
            x_1 \\ x_2
        \end{bmatrix}&=\begin{bmatrix}
            x_1 \\ x_2
        \end{bmatrix}
    \end{align*}
    This gives us the equations $2x_1+x_2=x_1$ and $x_1+2x_2=x_2$. These tell us that $x_1=-x_2$. Moreover, any vector that satisfies $\begin{pmatrix}
        a \\ -a
    \end{pmatrix}$ is an eigenvector of $A$ with eigenvalue $1$.

    Now let $\lambda=3$. We perform a similar analysis.
    \begin{align*}
        A\vec{x}&=\lambda\vec{x}\\
        A\vec{x}&=3\vec{x}\\
        \begin{bmatrix}
            2 & 1\\
            1 & 2
        \end{bmatrix}\begin{bmatrix}
            x_1 \\ x_2
        \end{bmatrix}&=\begin{bmatrix}
            3x_1 \\ 3x_2
        \end{bmatrix}
    \end{align*}
    This gives us the equations $2x_1+x_2=3x_1$ and $x_1+2x_2=3x_2$. Solving this system gives us that $x_1=x_2$. So the eigenvectors with eigenvalue $3$ are any vector that satisfy $\begin{pmatrix}
        a \\ a
    \end{pmatrix}$.
\end{example}
The previous example actually tells us something more about eigenvectors. We notice that when we solved the system, we got eigenvectors of a certain form not just one specific vector. In fact, once you have one eigenvector, you can generate many more by scaling. This leads to the following definition:
\begin{definition}
    The set of all eigenvectors of a matrix $A$ is called the \textit{eigenspace}. Note that this is a vector space so it necessarily includes the zero vector, but the zero vector is, by definition, not an eigenvector.
\end{definition}
\begin{remark}
    We discussed above how eigenvalues are exactly the solutions to the equation $(A-\lambda I)\vec{x}=\vec{0}$. This implies that the eigenspace is equivalent to the kernel of the transformation $A-\lambda I$.
\end{remark}
\begin{definition}
    When finding eigenvalues, we call the equation $\det(A-\lambda I)$ the characteristic polynomial.
\end{definition}
\begin{example}
    Let $T: \mathbb{P}^n\to\mathbb{P}^n$ be the derivative operator. What are the eigenvalues of $T$?

    First, we notice that if the input has degree $k\geq 1$, then the output has degree $k-1$. There is no way to scale a degree $k-1$ polynomial by a constant to get a degree $k$ polynomial thus there are no eigenvalues for polynomials of degree $k\geq 1$.

    This leaves the case when $k<1$. When $k<1$, our polynomial is simply a constant. The derivative of a constant is just $0$. Thus, this tells us that $T$ has eigenvalue $0$ which has eigenvectors that are any constant.
\end{example}
Before further discussion, we need to briefly discuss some algebra as solving the characteristic polynomial is the key component of finding eigenvalues.
\section{Diagonalization}
We previously discussed taking large powers of $A$. This can be very expensive especially if $A$ is high dimensional. However, if we have the special case where $A$ is diagonal, this computation is really easy since:
$$\begin{bmatrix}
    a & 0 & 0\\
    0 & b & 0\\
    0 & 0 & c
\end{bmatrix}\begin{bmatrix}
    a & 0 & 0\\
    0 & b & 0\\
    0 & 0 & c
\end{bmatrix}=\begin{bmatrix}
    a^2 & 0 & 0\\
    0 & b^2 & 0\\
    0 & 0 & c^2
\end{bmatrix}$$
Moreover, by induction we can see that:
$$\begin{bmatrix}
    a & 0 & 0\\
    0 & b & 0\\
    0 & 0 & c
\end{bmatrix}^n=\begin{bmatrix}
    a^n & 0 & 0\\
    0 & b^n & 0\\
    0 & 0 & c^n
\end{bmatrix}$$
\begin{definition}
    $T:V\to V$ is diagonalizable if there exists a basis $B$ for $V$ consisting of eigenvectors of $T$. Let $B=(\vec{v}_1,\ldots,\vec{v}_n)$ with corresponding eigenvalues $\lambda_1,\ldots,\lambda_n$ then:
    $$[T]_{BB}=\begin{bmatrix}
        \lambda_1 & 0 & 0 & 0\\
        0 & \lambda_2 & 0 & 0\\
        \vdots & \vdots & \vdots & \vdots\\
        0 & 0 & 0 & \lambda_n
    \end{bmatrix}$$
\end{definition}
Let's consider where this matrix comes from. We know that the first column of the matrix is equal to $[T(\vec{v}_1)]_B$. We also know that since $\vec{v}_1$ is an eigenvector, that $T(\vec{v}_1)=\lambda_1\vec{v}_1$. This gives us that $T(\vec{v}_1)=\lambda_1\vec{v}_1+0\vec{v}_2+0\vec{v_3}+\ldots+0\vec{v}_n$. This gives us the coordinate vector shown in the first column of the above matrix. You can apply similar logic to the rest of the columns.

Now, let's talk about how do we find this diagonal matrix. Based on the definition, we could find every single eigenvector and then check if they form a basis. If not, then $T$ is not diagonalizable. Otherwise, let's look at how we get $[T]_{BB}$. If we want, $[T]_{BB}$ we need to use change of basis matrices:
$$[T]_{BB}=[I]_{BE}[T]_{EE}[I]_{EB}$$
Note that $E$ represents the elementary basis. Note that initially, we only know $[T]_{EE}$.
\begin{align*}
    [T]_{BB}&=[I]_{BE}[T]_{EE}[I]_{EB}\\
    [I]_{BE}^{-1}[T]_{BB}&=[I]_{BE}^{-1}[I]_{BE}[T]_{EE}[I]_{EB}\\
    [I]_{BE}^{-1}[T]_{BB}&=[T]_{EE}[I]_{EB}\\
    [I]_{BE}^{-1}[T]_{BB}[I]_{EB}^{-1}&=[T]_{EE}[I]_{EB}[I]_{EB}^{-1}\\
    [I]_{BE}^{-1}[T]_{BB}[I]_{EB}^{-1}&=[T]_{EE}\\
    [I]_{EB}[T]_{BB}[I]_{BE}&=[T]_{EE}
\end{align*}
A common way this is notated is $P^{-1}DP=A$. Let's talk about why this achieves the efficient power multiplication that want.
\begin{align*}
    ([T]_{EE})^2&=([I]_{EB}[T]_{BB}[I]_{BE})^2\\
    &=([I]_{EB}[T]_{BB}[I]_{BE})([I]_{EB}[T]_{BB}[I]_{BE})\\
    &=([I]_{EB}[T]_{BB}[T]_{BB}[I]_{BE})\\
    &=([I]_{EB}[T]_{BB}^2[I]_{BE})
\end{align*}
By induction, this gives us:
$$([T]_{EE})^n=[I]_{EB}[T]_{BB}^n[I]_{BE}$$
Since $[T]_{BB}$ is diagonal, computing $[T]_{BB}^n$ is simple. Afterwards, we only need to perform two matrix multiplications. This is way more efficient than trying to directly compute $[T]_{EE}^n$ which would require $n$ difficult matrix multiplications.